#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨è®­ç»ƒå™¨æ¡†æ¶ - æ¨¡å‹æ— å…³çš„è®­ç»ƒæŠ½è±¡
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Callable
import time
import logging
import os
import sys
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.logger import TrainingLogger


class Trainer:
    """é€šç”¨è®­ç»ƒå™¨ç±» - æ”¯æŒä»»æ„PyTorchæ¨¡å‹çš„è®­ç»ƒ"""
    
    def __init__(self, 
                 model: nn.Module,
                 criterion: nn.Module,
                 optimizer: torch.optim.Optimizer,
                 device: torch.device,
                 scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
                 logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: PyTorchæ¨¡å‹
            criterion: æŸå¤±å‡½æ•°
            optimizer: ä¼˜åŒ–å™¨
            device: è®­ç»ƒè®¾å¤‡
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
            logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.model = model.to(device)
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device
        self.scheduler = scheduler
        self.logger = logger or logging.getLogger(__name__)
        
        # è®­ç»ƒå†å²è®°å½•
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
    
    def _is_moe_model(self) -> bool:
        """æ£€æµ‹æ¨¡å‹æ˜¯å¦æ˜¯MoEæ¨¡å‹ï¼ˆæœ‰compute_lossæ–¹æ³•ï¼‰"""
        return hasattr(self.model, 'compute_loss') and callable(getattr(self.model, 'compute_loss'))
    
    def train_epoch(self, train_loader: torch.utils.data.DataLoader, epoch: int, num_epochs: int) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epochç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰
            num_epochs: æ€»epochæ•°
            
        Returns:
            åŒ…å«æŸå¤±å’Œå‡†ç¡®ç‡çš„å­—å…¸
        """
        self.model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        # MoEæ¨¡å‹çš„é¢å¤–ç»Ÿè®¡
        is_moe = self._is_moe_model()
        expert_stats = None
        if is_moe:
            num_experts = getattr(self.model, 'num_experts', 0)
            num_layers = getattr(self.model, 'num_layers', len(self.model.layers)) if hasattr(self.model, 'layers') else 1
            if num_experts > 0:
                expert_stats = torch.zeros(num_layers, num_experts).to(self.device)
        
        num_batches = len(train_loader)
        start_time = time.time()
        
        print(f"\n{'='*50}")
        print(f"å¼€å§‹ç¬¬ {epoch+1}/{num_epochs} è½®è®­ç»ƒ")
        print(f"{'='*50}")
        
        for batch_idx, (data, targets) in enumerate(train_loader):
            # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            data, targets = data.to(self.device), targets.to(self.device)
            
            # æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„è°ƒç”¨æ–¹å¼
            if is_moe:
                # MoEæ¨¡å‹ï¼šè¿”å›logitså’Œé—¨æ§ä¿¡æ¯
                outputs, gate_info = self.model(data, return_gate=True)
                loss = self.model.compute_loss(outputs, targets, gate_info)
                
                # ç»Ÿè®¡ä¸“å®¶ä½¿ç”¨æƒ…å†µ
                if expert_stats is not None and 'expert_usage' in gate_info:
                    expert_stats += gate_info['expert_usage']
            else:
                # æ ‡å‡†æ¨¡å‹
                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            # ç»Ÿè®¡æŸå¤±å’Œå‡†ç¡®ç‡
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += targets.size(0)
            train_correct += predicted.eq(targets).sum().item()
            
            # æ‰“å°è¿›åº¦ - æ¯10%æ˜¾ç¤ºä¸€æ¬¡
            if batch_idx % max(1, num_batches // 10) == 0 or batch_idx == num_batches - 1:
                progress = (batch_idx + 1) / num_batches * 100
                avg_loss = train_loss / (batch_idx + 1)
                acc = 100. * train_correct / train_total
                elapsed_time = time.time() - start_time
                
                # æ˜¾ç¤ºè¿›åº¦æ¡
                bar_length = 30
                filled_length = int(bar_length * (batch_idx + 1) // num_batches)
                bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
                
                print(f'\rEpoch [{epoch+1}/{num_epochs}] |{bar}| {progress:.1f}% '
                      f'Loss: {loss.item():.4f}, Avg Loss: {avg_loss:.4f}, '
                      f'Acc: {acc:.2f}%, Time: {elapsed_time:.1f}s', end='')
        
        print()  # æ¢è¡Œ
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_train_loss = train_loss / num_batches
        train_acc = 100. * train_correct / train_total
        
        result = {
            'loss': avg_train_loss,
            'accuracy': train_acc
        }
        
        # å¦‚æœæ˜¯MoEæ¨¡å‹ï¼Œæ·»åŠ ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
        if is_moe and expert_stats is not None:
            # expert_stats: [num_layers, num_experts]
            avg_expert_usage = (expert_stats / num_batches).cpu()  # [num_layers, num_experts]
            result['expert_usage'] = avg_expert_usage.tolist()  # ä¿å­˜ä¸ºåˆ—è¡¨çš„åˆ—è¡¨
        
        return result
    
    def validate_epoch(self, val_loader: torch.utils.data.DataLoader) -> Dict[str, float]:
        """
        éªŒè¯ä¸€ä¸ªepoch
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            åŒ…å«æŸå¤±å’Œå‡†ç¡®ç‡çš„å­—å…¸
        """
        self.model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        # MoEæ¨¡å‹çš„é¢å¤–ç»Ÿè®¡
        is_moe = self._is_moe_model()
        expert_stats = None
        if is_moe:
            num_experts = getattr(self.model, 'num_experts', 0)
            num_layers = getattr(self.model, 'num_layers', len(self.model.layers)) if hasattr(self.model, 'layers') else 1
            if num_experts > 0:
                expert_stats = torch.zeros(num_layers, num_experts).to(self.device)
        
        print(f"å¼€å§‹éªŒè¯...")
        
        with torch.no_grad():
            val_batches = len(val_loader)
            for batch_idx, (data, targets) in enumerate(val_loader):
                # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                data, targets = data.to(self.device), targets.to(self.device)
                
                # å‰å‘ä¼ æ’­ - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„è°ƒç”¨æ–¹å¼
                if is_moe:
                    # MoEæ¨¡å‹ï¼šè¿”å›logitså’Œé—¨æ§ä¿¡æ¯
                    outputs, gate_info = self.model(data, return_gate=True)
                    loss = self.model.compute_loss(outputs, targets, gate_info)
                    
                    # ç»Ÿè®¡ä¸“å®¶ä½¿ç”¨æƒ…å†µ
                    if expert_stats is not None and 'expert_usage' in gate_info:
                        expert_stats += gate_info['expert_usage']
                else:
                    # æ ‡å‡†æ¨¡å‹
                    outputs = self.model(data)
                    loss = self.criterion(outputs, targets)
                
                # ç»Ÿè®¡æŸå¤±å’Œå‡†ç¡®ç‡
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()
                
                # æ˜¾ç¤ºéªŒè¯è¿›åº¦
                if batch_idx % max(1, val_batches // 5) == 0 or batch_idx == val_batches - 1:
                    progress = (batch_idx + 1) / val_batches * 100
                    bar_length = 20
                    filled_length = int(bar_length * (batch_idx + 1) // val_batches)
                    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
                    print(f'\réªŒè¯è¿›åº¦ |{bar}| {progress:.1f}%', end='')
        
        print()  # æ¢è¡Œ
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_val_loss = val_loss / val_batches
        val_acc = 100. * val_correct / val_total
        
        result = {
            'loss': avg_val_loss,
            'accuracy': val_acc
        }
        
        # å¦‚æœæ˜¯MoEæ¨¡å‹ï¼Œæ·»åŠ ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
        if is_moe and expert_stats is not None:
            # expert_stats: [num_layers, num_experts]
            avg_expert_usage = (expert_stats / val_batches).cpu()  # [num_layers, num_experts]
            result['expert_usage'] = avg_expert_usage.tolist()  # ä¿å­˜ä¸ºåˆ—è¡¨çš„åˆ—è¡¨
        
        return result
    
    def train(self, 
              train_loader: torch.utils.data.DataLoader,
              val_loader: torch.utils.data.DataLoader,
              num_epochs: int,
              callbacks: Optional[Dict[str, Callable]] = None) -> Dict[str, Any]:
        """
        å®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            callbacks: å›è°ƒå‡½æ•°å­—å…¸ï¼ˆå¯é€‰ï¼‰
                - on_epoch_end: æ¯ä¸ªepochç»“æŸæ—¶è°ƒç”¨
                - on_train_end: è®­ç»ƒç»“æŸæ—¶è°ƒç”¨
                
        Returns:
            è®­ç»ƒå†å²è®°å½•
        """
        self.logger.info(f"å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} è½®")
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        for epoch in range(num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_metrics = self.train_epoch(train_loader, epoch, num_epochs)
            
            # éªŒè¯é˜¶æ®µ
            val_metrics = self.validate_epoch(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler is not None:
                self.scheduler.step()
                current_lr = self.scheduler.get_last_lr()[0]
            else:
                current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_metrics['loss'])
            self.history['train_acc'].append(train_metrics['accuracy'])
            self.history['val_loss'].append(val_metrics['loss'])
            self.history['val_acc'].append(val_metrics['accuracy'])
            
            # æ‰“å°è½®æ¬¡ç»“æœ
            print(f'Epoch [{epoch+1}/{num_epochs}] ç»“æœ:')
            print(f'  è®­ç»ƒ - Loss: {train_metrics["loss"]:.4f}, Acc: {train_metrics["accuracy"]:.2f}%')
            print(f'  éªŒè¯ - Loss: {val_metrics["loss"]:.4f}, Acc: {val_metrics["accuracy"]:.2f}%')
            print(f'  å­¦ä¹ ç‡: {current_lr:.6f}')
            
            # å¦‚æœæ˜¯MoEæ¨¡å‹ï¼Œæ‰“å°ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
            if 'expert_usage' in train_metrics:
                expert_usage = train_metrics['expert_usage']  # [[layer1_exp1, layer1_exp2, ...], [layer2_exp1, ...]]
                num_layers = len(expert_usage)
                num_experts = len(expert_usage[0]) if num_layers > 0 else 0
                
                print(f'\n  ğŸ“Š MoEä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ï¼ˆè®­ç»ƒé›†ï¼‰:')
                
                # æ‰“å°æ¯å±‚çš„ä¸“å®¶ä½¿ç”¨æƒ…å†µ
                for layer_idx in range(num_layers):
                    print(f'\n    ç¬¬ {layer_idx+1} å±‚:')
                    for expert_idx in range(num_experts):
                        usage = expert_usage[layer_idx][expert_idx]
                        bar_len = int(usage * 50)
                        bar = 'â–ˆ' * bar_len + 'â–‘' * (50 - bar_len)
                        print(f'      ä¸“å®¶ {expert_idx+1}: [{bar}] {usage*100:.2f}%')
                
                # æ£€æµ‹ä¸“å®¶å¡Œé™·ï¼ˆå¹³å‡æ‰€æœ‰å±‚ï¼‰
                avg_usage_per_expert = [sum(expert_usage[l][e] for l in range(num_layers)) / num_layers 
                                       for e in range(num_experts)]
                expected = 1.0 / num_experts
                max_imbalance = max([abs(u - expected) for u in avg_usage_per_expert])
                
                if max_imbalance > 0.2:  # å¦‚æœåå·®è¶…è¿‡20%
                    print(f'\n     âš ï¸  è­¦å‘Š: ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡! æœ€å¤§åå·®={max_imbalance*100:.1f}%')
                if any(u < 0.05 for u in avg_usage_per_expert):  # å¦‚æœæœ‰ä¸“å®¶ä½¿ç”¨ç‡<5%
                    print(f'     âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°ä¸“å®¶å¡Œé™·! æŸäº›ä¸“å®¶å‡ ä¹ä¸è¢«ä½¿ç”¨')
            
            if 'expert_usage' in val_metrics:
                expert_usage = val_metrics['expert_usage']
                num_layers = len(expert_usage)
                num_experts = len(expert_usage[0]) if num_layers > 0 else 0
                
                print(f'\n  ğŸ“Š MoEä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ï¼ˆéªŒè¯é›†ï¼‰:')
                
                for layer_idx in range(num_layers):
                    print(f'\n    ç¬¬ {layer_idx+1} å±‚:')
                    for expert_idx in range(num_experts):
                        usage = expert_usage[layer_idx][expert_idx]
                        bar_len = int(usage * 50)
                        bar = 'â–ˆ' * bar_len + 'â–‘' * (50 - bar_len)
                        print(f'      ä¸“å®¶ {expert_idx+1}: [{bar}] {usage*100:.2f}%')
            
            log_msg = (f'Epoch [{epoch+1}/{num_epochs}] - '
                      f'Train Loss: {train_metrics["loss"]:.4f}, Train Acc: {train_metrics["accuracy"]:.2f}%, '
                      f'Val Loss: {val_metrics["loss"]:.4f}, Val Acc: {val_metrics["accuracy"]:.2f}%')
            if 'expert_usage' in train_metrics:
                # è®°å½•ä¸“å®¶ä½¿ç”¨è¯¦æƒ…ï¼ˆå¹³å‡æ‰€æœ‰å±‚ï¼‰
                expert_usage = train_metrics['expert_usage']
                num_layers = len(expert_usage)
                num_experts = len(expert_usage[0]) if num_layers > 0 else 0
                avg_usage_per_expert = [sum(expert_usage[l][e] for l in range(num_layers)) / num_layers 
                                       for e in range(num_experts)]
                expert_usage_str = ', '.join([f'E{i+1}:{u*100:.1f}%' for i, u in enumerate(avg_usage_per_expert)])
                log_msg += f', Expert Usage: [{expert_usage_str}]'
            self.logger.info(log_msg)
            
            # æ‰§è¡Œå›è°ƒå‡½æ•°
            if callbacks and 'on_epoch_end' in callbacks:
                callbacks['on_epoch_end'](epoch, train_metrics, val_metrics)
        
        # æ‰§è¡Œè®­ç»ƒç»“æŸå›è°ƒ
        if callbacks and 'on_train_end' in callbacks:
            callbacks['on_train_end'](self.history)
        
        self.logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
        print("\næ¨¡å‹è®­ç»ƒå®Œæˆ!")
        
        return self.history
    
    def predict(self, test_loader: torch.utils.data.DataLoader):
        """
        åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            
        Returns:
            é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
        """
        self.model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for data, targets in test_loader:
                # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                data = data.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(data)
                
                # è·å–é¢„æµ‹ç»“æœ
                _, predicted = outputs.max(1)
                
                # æ”¶é›†ç»“æœ
                predictions.extend(predicted.cpu().numpy())
                true_labels.extend(targets.numpy())
        
        import numpy as np
        return np.array(predictions), np.array(true_labels)
    
    def evaluate(self, y_true: np.ndarray, y_pred: np.ndarray, class_names: list = None) -> Dict[str, Any]:
        """
        è¯„ä¼°é¢„æµ‹ç»“æœ
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ ‡ç­¾
            class_names: ç±»åˆ«åç§°
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        
        # æ„å»ºç»“æœå­—å…¸
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': cm
        }
        
        # å¦‚æœæä¾›äº†ç±»åˆ«åç§°ï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        if class_names:
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°
            precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(y_true, y_pred)
            
            class_metrics = {}
            for i, class_name in enumerate(class_names):
                if i < len(precision_per_class):
                    class_metrics[class_name] = {
                        'precision': precision_per_class[i],
                        'recall': recall_per_class[i],
                        'f1_score': f1_per_class[i]
                    }
            
            results['class_metrics'] = class_metrics
        
        return results
    
    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save(self.model.state_dict(), path)
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {path}")
    
    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        self.model.load_state_dict(torch.load(path))
        self.logger.info(f"æ¨¡å‹å·²ä» {path} åŠ è½½")


class TrainerBuilder:
    """è®­ç»ƒå™¨æ„å»ºå™¨ - ç”¨äºç®€åŒ–è®­ç»ƒå™¨çš„åˆ›å»º"""
    
    def __init__(self, model: nn.Module, model_name: str = 'model', task_type: str = 'task'):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨æ„å»ºå™¨
        
        Args:
            model: PyTorchæ¨¡å‹
            model_name: æ¨¡å‹åç§°ï¼Œç”¨äºæ—¥å¿—æ–‡ä»¶å‘½å
            task_type: ä»»åŠ¡ç±»å‹ï¼Œç”¨äºæ—¥å¿—æ–‡ä»¶å‘½å
        """
        self.model = model
        self.model_name = model_name
        self.task_type = task_type
        self._criterion = None
        self._optimizer = None
        self._scheduler = None
        self._device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self._logger = None
        self._use_custom_logger = True
    
    def with_criterion(self, criterion: nn.Module):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        self._criterion = criterion
        return self
    
    def with_optimizer(self, optimizer_class, **kwargs):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        self._optimizer = optimizer_class(self.model.parameters(), **kwargs)
        return self
    
    def with_scheduler(self, scheduler_class, **kwargs):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self._optimizer is None:
            raise ValueError("å¿…é¡»å…ˆè®¾ç½®ä¼˜åŒ–å™¨æ‰èƒ½è®¾ç½®è°ƒåº¦å™¨")
        self._scheduler = scheduler_class(self._optimizer, **kwargs)
        return self
    
    def with_device(self, device: torch.device):
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        self._device = device
        return self
    
    def with_logger(self, logger: logging.Logger):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨ï¼ˆå¤–éƒ¨æä¾›ï¼‰"""
        self._logger = logger
        self._use_custom_logger = False
        return self
    
    def with_auto_logger(self, log_dir: str = './logs'):
        """ä½¿ç”¨è‡ªåŠ¨åˆ›å»ºçš„è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""
        logger_manager = TrainingLogger(log_dir=log_dir)
        self._logger = logger_manager.setup_logger(self.model_name, self.task_type)
        self._use_custom_logger = True
        return self
    
    def build(self) -> Trainer:
        """æ„å»ºè®­ç»ƒå™¨"""
        if self._criterion is None:
            raise ValueError("å¿…é¡»è®¾ç½®æŸå¤±å‡½æ•°")
        if self._optimizer is None:
            raise ValueError("å¿…é¡»è®¾ç½®ä¼˜åŒ–å™¨")
        
        # å¦‚æœæ²¡æœ‰è®¾ç½®loggerï¼Œè‡ªåŠ¨åˆ›å»ºä¸€ä¸ª
        if self._logger is None and self._use_custom_logger:
            logger_manager = TrainingLogger()
            self._logger = logger_manager.setup_logger(self.model_name, self.task_type)
        
        return Trainer(
            model=self.model,
            criterion=self._criterion,
            optimizer=self._optimizer,
            device=self._device,
            scheduler=self._scheduler,
            logger=self._logger
        )


if __name__ == "__main__":
    print("é€šç”¨è®­ç»ƒå™¨æ¡†æ¶æ¨¡å—")
    print("ä½¿ç”¨ç¤ºä¾‹:")
    print("""
    # åˆ›å»ºæ¨¡å‹
    model = YourModel()
    
    # ä½¿ç”¨æ„å»ºå™¨åˆ›å»ºè®­ç»ƒå™¨
    trainer = TrainerBuilder(model) \\
        .with_criterion(nn.CrossEntropyLoss()) \\
        .with_optimizer(torch.optim.Adam, lr=0.001) \\
        .with_scheduler(torch.optim.lr_scheduler.StepLR, step_size=5, gamma=0.5) \\
        .with_device(torch.device('cuda')) \\
        .build()
    
    # å¼€å§‹è®­ç»ƒ
    history = trainer.train(train_loader, val_loader, num_epochs=10)
    
    # é¢„æµ‹
    predictions, labels = trainer.predict(test_loader)
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model('model.pth')
    """)