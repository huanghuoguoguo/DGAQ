#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DGA Detection - ç»Ÿä¸€æ•°æ®é›†å¤„ç†æ¨¡å—
"""

import torch
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import pickle
import os
from typing import Tuple, Dict, Any


class DGADataset(Dataset):
    """ç»Ÿä¸€çš„DGAæ•°æ®é›†ç±»"""
    
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


def load_dataset(file_path: str = './data/processed/small_dga_dataset.pkl') -> Dict[str, Any]:
    """åŠ è½½é¢„å¤„ç†çš„æ•°æ®é›†"""
    if not os.path.exists(file_path):
        # å°è¯•åŸå§‹ä½ç½®
        fallback_path = './data/small_dga_dataset.pkl'
        if os.path.exists(fallback_path):
            file_path = fallback_path
        else:
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with open(file_path, 'rb') as f:
        dataset = pickle.load(f)
    return dataset


def create_data_loaders(dataset_path: str = './data/processed/small_dga_dataset.pkl',
                       batch_size: int = 32,
                       train_ratio: float = 0.7,
                       val_ratio: float = 0.15,
                       random_seed: int = 42,
                       task_type: str = 'binary') -> Tuple[DataLoader, DataLoader, DataLoader, Dict[str, Any]]:
    """åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(dataset_path)
    
    # æ£€æŸ¥æ•°æ®é›†ç»“æ„ç±»å‹
    if 'train' in dataset and 'val' in dataset and 'test' in dataset:
        # ç»Ÿä¸€æ•°æ®é›†æ ¼å¼
        train_data = dataset['train']
        val_data = dataset['val']
        test_data = dataset['test']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰infoæˆ–metadata
        if 'info' in dataset:
            metadata = dataset['info']
        elif 'metadata' in dataset:
            metadata = dataset['metadata']
        else:
            raise KeyError("æ•°æ®é›†ç¼ºå°‘metadataä¿¡æ¯")
        
        # å¤„ç†åŸŸååºåˆ—åŒ–
        if 'sequences' in train_data:
            # å·²åºåˆ—åŒ–çš„æ•°æ®
            X_train = np.array(train_data['sequences'])
            X_val = np.array(val_data['sequences'])
            X_test = np.array(test_data['sequences'])
        elif 'domains' in train_data:
            # éœ€è¦åºåˆ—åŒ–çš„åŸŸåæ•°æ®
            # åˆ›å»ºå­—ç¬¦æ˜ å°„
            chars = set()
            for domains in [train_data['domains'], val_data['domains'], test_data['domains']]:
                for domain in domains:
                    chars.update(domain.lower())
            
            # æ„å»ºå­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„
            char_to_idx = {'<PAD>': 0, '<UNK>': 1}
            for i, char in enumerate(sorted(chars), 2):
                char_to_idx[char] = i
            
            def domain_to_sequence(domain: str, max_len: int = 60):
                sequence = [char_to_idx.get(char.lower(), char_to_idx['<UNK>']) for char in domain]
                if len(sequence) > max_len:
                    sequence = sequence[:max_len]
                else:
                    sequence.extend([char_to_idx['<PAD>']] * (max_len - len(sequence)))
                return sequence
            
            max_length = 60  # é»˜è®¤æœ€å¤§é•¿åº¦
            X_train = np.array([domain_to_sequence(domain, max_length) for domain in train_data['domains']])
            X_val = np.array([domain_to_sequence(domain, max_length) for domain in val_data['domains']])
            X_test = np.array([domain_to_sequence(domain, max_length) for domain in test_data['domains']])
            
            # æ›´æ–°è¯æ±‡è¡¨å¤§å°
            vocab_size = len(char_to_idx)
        else:
            raise KeyError("æ•°æ®é›†ç¼ºå°‘åŸŸåæ•°æ®")
        
        # è½¬æ¢æ ‡ç­¾ä¸ºnumpyæ•°ç»„
        y_train = np.array(train_data['labels'])
        y_val = np.array(val_data['labels'])
        y_test = np.array(test_data['labels'])
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è½¬æ¢æ ‡ç­¾
        if task_type == 'binary':
            # äºŒåˆ†ç±»ï¼š0=è‰¯æ€§ï¼Œ1=æ¶æ„ï¼ˆæ‰€æœ‰é0æ ‡ç­¾è½¬ä¸º1ï¼‰
            y_train = (y_train > 0).astype(int)
            y_val = (y_val > 0).astype(int)
            y_test = (y_test > 0).astype(int)
            num_classes = 2
            class_names = ['benign', 'malicious']
        else:
            # å¤šåˆ†ç±»ï¼šä¿æŒåŸå§‹æ ‡ç­¾ï¼Œç±»åˆ«æ•°åº”è¯¥æ˜¯æœ€å¤§æ ‡ç­¾å€¼+1
            all_labels = np.concatenate([y_train, y_val, y_test])
            num_classes = int(np.max(all_labels)) + 1
            
            # å°è¯•ä»metadataä¸­è·å–çœŸå®çš„ç±»åˆ«åç§°
            if 'label_mapping' in metadata:
                # ä½¿ç”¨label_mappingæ„å»ºclass_names
                label_to_name = {v: k for k, v in metadata['label_mapping'].items()}
                class_names = [label_to_name.get(i, f'unknown_{i}') for i in range(num_classes)]
            elif 'malicious_families' in metadata:
                # ä½¿ç”¨malicious_familiesæ„å»ºclass_names
                class_names = ['benign'] + metadata['malicious_families'][:num_classes-1]
            else:
                # é™çº§åˆ°é€šç”¨å‘½å
                class_names = ['benign'] + [f'malicious_{i}' for i in range(1, num_classes)]
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
        train_dataset = DGADataset(X_train, y_train)
        val_dataset = DGADataset(X_val, y_val)
        test_dataset = DGADataset(X_test, y_test)
        
        # æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'vocab_size': vocab_size if 'vocab_size' in locals() else metadata.get('vocab_size', 128),
            'max_length': metadata.get('max_length', X_train.shape[1]),
            'num_classes': num_classes,
            'class_names': class_names,
            'total_samples': len(X_train) + len(X_val) + len(X_test),
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'task_type': task_type,
            'class_distribution': {
                'train': np.bincount(y_train),
                'val': np.bincount(y_val),
                'test': np.bincount(y_test)
            }
        }
        
    elif 'X_train' in dataset:
        # æ—§æ ¼å¼çš„å¤šåˆ†ç±»æ•°æ®é›†
        X_train, y_train = dataset['X_train'], dataset['y_train']
        X_val, y_val = dataset['X_val'], dataset['y_val']
        X_test, y_test = dataset['X_test'], dataset['y_test']
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
        train_dataset = DGADataset(X_train, y_train)
        val_dataset = DGADataset(X_val, y_val)
        test_dataset = DGADataset(X_test, y_test)
        
        # æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'vocab_size': dataset['vocab_size'],
            'max_length': dataset.get('max_length', X_train.shape[1]),
            'num_classes': dataset.get('num_classes', 2),
            'class_names': dataset.get('class_names', ['benign', 'malicious']),
            'total_samples': len(X_train) + len(X_val) + len(X_test),
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'class_distribution': np.bincount(y_train)
        }
        
    else:
        # æ—§æ ¼å¼æ•°æ®é›†éœ€è¦åˆ†å‰²
        X, y = dataset['X'], dataset['y']
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è½¬æ¢æ ‡ç­¾
        if task_type == 'binary':
            # äºŒåˆ†ç±»ï¼š0=è‰¯æ€§ï¼Œ1=æ¶æ„ï¼ˆæ‰€æœ‰é0æ ‡ç­¾è½¬ä¸º1ï¼‰
            y = (y > 0).astype(int)
            num_classes = 2
            class_names = ['benign', 'malicious']
        else:
            # å¤šåˆ†ç±»ï¼šä¿æŒåŸå§‹æ ‡ç­¾
            num_classes = len(np.unique(y))
            class_names = ['benign'] + [f'malicious_{i}' for i in range(1, num_classes)]
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
        full_dataset = DGADataset(X, y)
        
        # è®¡ç®—åˆ’åˆ†å¤§å°
        total_size = len(full_dataset)
        train_size = int(train_ratio * total_size)
        val_size = int(val_ratio * total_size)
        test_size = total_size - train_size - val_size
        
        # æ•°æ®é›†åˆ’åˆ†
        train_dataset, val_dataset, test_dataset = random_split(
            full_dataset, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(random_seed)
        )
        
        # æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'vocab_size': dataset['vocab_size'],
            'max_length': dataset.get('max_length', X.shape[1]),
            'num_classes': num_classes,
            'class_names': class_names,
            'total_samples': total_size,
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'task_type': task_type,
            'class_distribution': np.bincount(y)
        }
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä¼˜åŒ–GPUåˆ©ç”¨ç‡
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=4,  # å¤šè¿›ç¨‹æ•°æ®åŠ è½½
        pin_memory=True,  # å†…å­˜å›ºå®šï¼ŒåŠ é€ŸGPUä¼ è¾“
        persistent_workers=True  # ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        persistent_workers=True
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        persistent_workers=True
    )
    
    return train_loader, val_loader, test_loader, dataset_info


def print_dataset_info(dataset_info: Dict[str, Any]):
    """æ‰“å°æ•°æ®é›†ä¿¡æ¯"""
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ€»æ ·æœ¬æ•°: {dataset_info['total_samples']}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {dataset_info['vocab_size']}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {dataset_info['max_length']}")
    print(f"  ç±»åˆ«æ•°: {dataset_info['num_classes']}")
    print(f"  ç±»åˆ«åç§°: {dataset_info['class_names'][:5]}... (å…±{len(dataset_info['class_names'])}ä¸ª)")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {dataset_info['class_distribution']}")
    print(f"  è®­ç»ƒé›†: {dataset_info['train_samples']}")
    print(f"  éªŒè¯é›†: {dataset_info['val_samples']}")
    print(f"  æµ‹è¯•é›†: {dataset_info['test_samples']}")


if __name__ == "__main__":
    # æµ‹è¯•åŠ è½½500kæ•°æ®é›†å¹¶æ£€æŸ¥ç±»åˆ«åç§°
    print("æµ‹è¯•åŠ è½½500kæ•°æ®é›†...")
    train_loader, val_loader, test_loader, dataset_info = create_data_loaders(
        dataset_path='./data/processed/500k_unified_dga_dataset.pkl',
        batch_size=32,
        task_type='multiclass'
    )
    
    print(f"\nâœ… ç±»åˆ«æ€»æ•°: {dataset_info['num_classes']}")
    print(f"\nğŸ¯ æ‰€æœ‰ç±»åˆ«åç§°:")
    for i, name in enumerate(dataset_info['class_names']):
        print(f"  {i:2d}: {name}")