"""
GANè®­ç»ƒç»“æœåˆ†æè„šæœ¬
ç”¨äºç”Ÿæˆæ ·æœ¬ã€è¯„ä¼°è´¨é‡å¹¶å¯è§†åŒ–ç»“æœ
"""
import torch
import numpy as np
from core.adversarial.generator import DGAGenerator
from core.dataset import create_data_loaders
import argparse

# å­—ç¬¦æ˜ å°„è¡¨ï¼ˆéœ€è¦ä¸dataset.pyä¿æŒä¸€è‡´ï¼‰
CHARS = "abcdefghijklmnopqrstuvwxyz0123456789-."
CHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}
IDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}
PAD_IDX = len(CHARS)
UNK_IDX = len(CHARS) + 1

def indices_to_domain(indices):
    """å°†ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºåŸŸåå­—ç¬¦ä¸²"""
    domain = ""
    for idx in indices:
        if idx == PAD_IDX or idx >= len(CHARS):
            break
        domain += IDX_TO_CHAR.get(idx, "?")
    return domain

def analyze_generated_samples(generator, device, num_samples=50):
    """ç”Ÿæˆå¹¶åˆ†ææ ·æœ¬"""
    print(f"\n{'='*80}")
    print(f"ğŸ² ç”Ÿæˆ {num_samples} ä¸ªDGAåŸŸåæ ·æœ¬")
    print(f"{'='*80}\n")
    
    # ç”Ÿæˆæ ·æœ¬
    indices = generator.sample(num_samples, device)
    indices_np = indices.cpu().numpy()
    
    generated_domains = []
    lengths = []
    
    print("ç”Ÿæˆçš„åŸŸåç¤ºä¾‹ï¼ˆå‰20ä¸ªï¼‰:")
    print("-" * 80)
    for i in range(min(20, num_samples)):
        domain = indices_to_domain(indices_np[i])
        generated_domains.append(domain)
        lengths.append(len(domain))
        print(f"{i+1:3d}. {domain:40s} (é•¿åº¦: {len(domain)})")
    
    # ç»Ÿè®¡åˆ†æ
    print(f"\n{'='*80}")
    print("ğŸ“Š ç»Ÿè®¡åˆ†æ")
    print(f"{'='*80}")
    
    all_lengths = [len(indices_to_domain(indices_np[i])) for i in range(num_samples)]
    
    print(f"å¹³å‡é•¿åº¦: {np.mean(all_lengths):.2f}")
    print(f"æœ€å°é•¿åº¦: {np.min(all_lengths)}")
    print(f"æœ€å¤§é•¿åº¦: {np.max(all_lengths)}")
    print(f"æ ‡å‡†å·®: {np.std(all_lengths):.2f}")
    
    # å­—ç¬¦åˆ†å¸ƒç»Ÿè®¡
    char_counts = {c: 0 for c in CHARS}
    for domain in generated_domains:
        for c in domain:
            if c in char_counts:
                char_counts[c] += 1
    
    print(f"\nå­—ç¬¦ä½¿ç”¨é¢‘ç‡ï¼ˆTop 10ï¼‰:")
    sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)
    for char, count in sorted_chars[:10]:
        print(f"  '{char}': {count} æ¬¡")
    
    # è´¨é‡è¯„ä¼°
    print(f"\n{'='*80}")
    print("ğŸ” è´¨é‡è¯„ä¼°")
    print(f"{'='*80}")
    
    # 1. æœ‰æ•ˆåŸŸåæ¯”ä¾‹ï¼ˆéç©ºä¸”ä¸å…¨æ˜¯ç‰¹æ®Šå­—ç¬¦ï¼‰
    valid_count = sum(1 for d in generated_domains if len(d) > 3 and any(c.isalnum() for c in d))
    print(f"æœ‰æ•ˆåŸŸåæ¯”ä¾‹: {valid_count}/{len(generated_domains)} ({100*valid_count/len(generated_domains):.1f}%)")
    
    # 2. åŒ…å«æ•°å­—çš„åŸŸåæ¯”ä¾‹
    with_digits = sum(1 for d in generated_domains if any(c.isdigit() for c in d))
    print(f"åŒ…å«æ•°å­—çš„åŸŸå: {with_digits}/{len(generated_domains)} ({100*with_digits/len(generated_domains):.1f}%)")
    
    # 3. åŒ…å«è¿å­—ç¬¦çš„åŸŸåæ¯”ä¾‹
    with_hyphen = sum(1 for d in generated_domains if '-' in d)
    print(f"åŒ…å«è¿å­—ç¬¦çš„åŸŸå: {with_hyphen}/{len(generated_domains)} ({100*with_hyphen/len(generated_domains):.1f}%)")
    
    # 4. ç†µå€¼åˆ†æï¼ˆå¤šæ ·æ€§ï¼‰
    unique_domains = len(set(generated_domains))
    print(f"å”¯ä¸€åŸŸåæ•°é‡: {unique_domains}/{len(generated_domains)} ({100*unique_domains/len(generated_domains):.1f}%)")
    
    return generated_domains

def main():
    parser = argparse.ArgumentParser(description="Analyze GAN Training Results")
    parser.add_argument('--model_path', type=str, required=True, help='Path to generator model')
    parser.add_argument('--vocab_size', type=int, default=40, help='Vocabulary size')
    parser.add_argument('--max_len', type=int, default=60, help='Max sequence length')
    parser.add_argument('--hidden_dim', type=int, default=256, help='Hidden dimension')
    parser.add_argument('--z_dim', type=int, default=100, help='Latent dimension')
    parser.add_argument('--num_samples', type=int, default=50, help='Number of samples to generate')
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½ç”Ÿæˆå™¨
    print(f"\nåŠ è½½ç”Ÿæˆå™¨æ¨¡å‹: {args.model_path}")
    generator = DGAGenerator(
        vocab_size=args.vocab_size,
        hidden_dim=args.hidden_dim,
        max_len=args.max_len,
        z_dim=args.z_dim
    ).to(device)
    
    generator.load_state_dict(torch.load(args.model_path, map_location=device))
    generator.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åˆ†æç”Ÿæˆæ ·æœ¬
    generated_domains = analyze_generated_samples(generator, device, args.num_samples)
    
    # ä¿å­˜ç»“æœ
    output_file = "gan_generated_samples.txt"
    with open(output_file, 'w') as f:
        for domain in generated_domains:
            f.write(domain + '\n')
    print(f"\nâœ… ç”Ÿæˆçš„åŸŸåå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()
