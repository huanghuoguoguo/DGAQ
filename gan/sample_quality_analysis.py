#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GANç”Ÿæˆæ ·æœ¬è´¨é‡åˆ†æ
è¯„ä¼°ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬ä¸çœŸå®DGAæ ·æœ¬çš„ç›¸ä¼¼åº¦
"""

import os
import sys
import torch
import numpy as np
from collections import Counter
import argparse

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from core.adversarial.generator import DGAGenerator
from core.dataset import load_dataset
from core.model.cnn_model import LightweightCNN

def load_generator(model_path, device, vocab_size=41, hidden_dim=256, max_len=60, z_dim=100):
    """åŠ è½½ç”Ÿæˆå™¨"""
    generator = DGAGenerator(vocab_size=vocab_size, hidden_dim=hidden_dim, 
                            max_len=max_len, z_dim=z_dim).to(device)
    generator.load_state_dict(torch.load(model_path, map_location=device, weights_only=False))
    generator.eval()
    return generator

def analyze_sequence_distribution(sequences):
    """åˆ†æåºåˆ—ç»Ÿè®¡ç‰¹å¾"""
    sequences_np = sequences.cpu().numpy() if torch.is_tensor(sequences) else sequences
    
    # æœ‰æ•ˆé•¿åº¦ï¼ˆépaddingéƒ¨åˆ†ï¼‰
    lengths = []
    for seq in sequences_np:
        length = np.sum(seq != 0)
        lengths.append(length)
    
    # å­—ç¬¦åˆ†å¸ƒ
    all_chars = sequences_np[sequences_np != 0]
    char_dist = Counter(all_chars)
    
    # å”¯ä¸€å­—ç¬¦æ•°
    unique_chars = len(char_dist)
    
    return {
        'avg_length': np.mean(lengths),
        'std_length': np.std(lengths),
        'min_length': np.min(lengths),
        'max_length': np.max(lengths),
        'unique_chars': unique_chars,
        'char_distribution': dict(sorted(char_dist.items())[:10]),  # Top 10
        'total_chars': len(all_chars)
    }

def main():
    parser = argparse.ArgumentParser(description="åˆ†æGANç”Ÿæˆæ ·æœ¬è´¨é‡")
    parser.add_argument('--generator_path', type=str, 
                       default='./models/gan/generator_epoch_10.pth')
    parser.add_argument('--dataset_path', type=str,
                       default='./data/processed/500k_unified_dga_dataset.pkl')
    parser.add_argument('--target_model_path', type=str,
                       default='./models/cnn_binary_model.pth')
    parser.add_argument('--num_samples', type=int, default=5000)
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}\n")
    
    # åŠ è½½æ•°æ®é›†
    print("åŠ è½½çœŸå®æ•°æ®é›†...")
    dataset = load_dataset(args.dataset_path)
    
    # è·å–çœŸå®DGAæ ·æœ¬ï¼ˆæ ‡ç­¾>0çš„ä¸ºæ¶æ„ï¼‰
    from core.dataset import create_data_loaders
    _, _, test_loader, info = create_data_loaders(args.dataset_path, 
                                                   batch_size=args.num_samples,
                                                   task_type='binary')
    
    # è·å–ä¸€æ‰¹æµ‹è¯•æ•°æ®
    for real_seqs, labels in test_loader:
        # åªå–æ¶æ„æ ·æœ¬
        malicious_mask = labels == 1
        real_dga_samples = real_seqs[malicious_mask][:args.num_samples]
        benign_samples = real_seqs[labels == 0][:args.num_samples]
        break
    
    print(f"çœŸå®DGAæ ·æœ¬æ•°: {len(real_dga_samples)}")
    print(f"çœŸå®è‰¯æ€§æ ·æœ¬æ•°: {len(benign_samples)}\n")
    
    # åŠ è½½ç”Ÿæˆå™¨
    print(f"åŠ è½½ç”Ÿæˆå™¨: {args.generator_path}")
    generator = load_generator(args.generator_path, device)
    
    # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
    print(f"ç”Ÿæˆ {args.num_samples} ä¸ªå¯¹æŠ—æ ·æœ¬...")
    with torch.no_grad():
        generated_samples = generator.sample(args.num_samples, device)
    
    # åŠ è½½ç›®æ ‡åˆ†ç±»å™¨
    print(f"åŠ è½½ç›®æ ‡åˆ†ç±»å™¨: {args.target_model_path}\n")
    target_model = LightweightCNN(vocab_size=41, embedding_dim=128, 
                                   max_length=60, num_classes=2).to(device)
    target_model.load_state_dict(torch.load(args.target_model_path, 
                                             map_location=device, weights_only=False))
    target_model.eval()
    
    # åˆ†æç»Ÿè®¡ç‰¹å¾
    print("="*70)
    print("ğŸ“Š æ ·æœ¬ç»Ÿè®¡ç‰¹å¾å¯¹æ¯”")
    print("="*70)
    
    gen_stats = analyze_sequence_distribution(generated_samples)
    real_dga_stats = analyze_sequence_distribution(real_dga_samples)
    benign_stats = analyze_sequence_distribution(benign_samples)
    
    print(f"\n{'æŒ‡æ ‡':<20} {'ç”Ÿæˆæ ·æœ¬':>15} {'çœŸå®DGA':>15} {'çœŸå®è‰¯æ€§':>15}")
    print("-"*70)
    print(f"{'å¹³å‡é•¿åº¦':<20} {gen_stats['avg_length']:>15.2f} {real_dga_stats['avg_length']:>15.2f} {benign_stats['avg_length']:>15.2f}")
    print(f"{'é•¿åº¦æ ‡å‡†å·®':<20} {gen_stats['std_length']:>15.2f} {real_dga_stats['std_length']:>15.2f} {benign_stats['std_length']:>15.2f}")
    print(f"{'æœ€å°é•¿åº¦':<20} {gen_stats['min_length']:>15.0f} {real_dga_stats['min_length']:>15.0f} {benign_stats['min_length']:>15.0f}")
    print(f"{'æœ€å¤§é•¿åº¦':<20} {gen_stats['max_length']:>15.0f} {real_dga_stats['max_length']:>15.0f} {benign_stats['max_length']:>15.0f}")
    print(f"{'å”¯ä¸€å­—ç¬¦æ•°':<20} {gen_stats['unique_chars']:>15} {real_dga_stats['unique_chars']:>15} {benign_stats['unique_chars']:>15}")
    
    # åˆ†ç±»å™¨é¢„æµ‹åˆ†å¸ƒ
    print(f"\n{'='*70}")
    print("ğŸ¯ åˆ†ç±»å™¨é¢„æµ‹åˆ†æ")
    print("="*70)
    
    with torch.no_grad():
        gen_logits = target_model(generated_samples)
        real_dga_logits = target_model(real_dga_samples.to(device))
        benign_logits = target_model(benign_samples.to(device))
        
        gen_probs = torch.softmax(gen_logits, dim=1)
        real_dga_probs = torch.softmax(real_dga_logits, dim=1)
        benign_probs = torch.softmax(benign_logits, dim=1)
        
        gen_preds = torch.argmax(gen_probs, dim=1)
        real_dga_preds = torch.argmax(real_dga_probs, dim=1)
        benign_preds = torch.argmax(benign_probs, dim=1)
    
    print(f"\n{'æ ·æœ¬ç±»å‹':<20} {'é¢„æµ‹ä¸ºè‰¯æ€§':>15} {'é¢„æµ‹ä¸ºæ¶æ„':>15} {'å‡†ç¡®ç‡/ASR':>15}")
    print("-"*70)
    
    gen_benign = (gen_preds == 0).sum().item()
    gen_malicious = (gen_preds == 1).sum().item()
    asr = gen_benign / len(generated_samples) * 100
    print(f"{'ç”Ÿæˆæ ·æœ¬':<20} {gen_benign:>15} {gen_malicious:>15} {asr:>14.2f}%")
    
    dga_benign = (real_dga_preds == 0).sum().item()
    dga_malicious = (real_dga_preds == 1).sum().item()
    dga_acc = dga_malicious / len(real_dga_samples) * 100
    print(f"{'çœŸå®DGA':<20} {dga_benign:>15} {dga_malicious:>15} {dga_acc:>14.2f}%")
    
    ben_benign = (benign_preds == 0).sum().item()
    ben_malicious = (benign_preds == 1).sum().item()
    ben_acc = ben_benign / len(benign_samples) * 100
    print(f"{'çœŸå®è‰¯æ€§':<20} {ben_benign:>15} {ben_malicious:>15} {ben_acc:>14.2f}%")
    
    # ç½®ä¿¡åº¦åˆ†æ
    print(f"\n{'æ ·æœ¬ç±»å‹':<20} {'è‰¯æ€§ç½®ä¿¡åº¦':>15} {'æ¶æ„ç½®ä¿¡åº¦':>15}")
    print("-"*70)
    print(f"{'ç”Ÿæˆæ ·æœ¬':<20} {gen_probs[:, 0].mean().item():>15.4f} {gen_probs[:, 1].mean().item():>15.4f}")
    print(f"{'çœŸå®DGA':<20} {real_dga_probs[:, 0].mean().item():>15.4f} {real_dga_probs[:, 1].mean().item():>15.4f}")
    print(f"{'çœŸå®è‰¯æ€§':<20} {benign_probs[:, 0].mean().item():>15.4f} {benign_probs[:, 1].mean().item():>15.4f}")
    
    # æ€»ç»“ä¸å»ºè®®
    print(f"\n{'='*70}")
    print("ğŸ“ åˆ†ææ€»ç»“ä¸å»ºè®®")
    print("="*70)
    
    print(f"\nâœ… æˆåŠŸæŒ‡æ ‡:")
    print(f"  - æ”»å‡»æˆåŠŸç‡ (ASR): {asr:.2f}%")
    print(f"  - ç”Ÿæˆæ ·æœ¬å¹³å‡é•¿åº¦: {gen_stats['avg_length']:.1f} (çœŸå®DGA: {real_dga_stats['avg_length']:.1f})")
    
    print(f"\nâš ï¸ å‘ç°çš„é—®é¢˜:")
    if gen_stats['avg_length'] < real_dga_stats['avg_length'] * 0.5:
        print(f"  - ç”Ÿæˆåºåˆ—è¿‡çŸ­ï¼Œå¹³å‡é•¿åº¦ä»…ä¸ºçœŸå®DGAçš„ {gen_stats['avg_length']/real_dga_stats['avg_length']*100:.1f}%")
    if gen_stats['unique_chars'] < real_dga_stats['unique_chars']:
        print(f"  - å­—ç¬¦å¤šæ ·æ€§ä¸è¶³ï¼Œä»…ä½¿ç”¨ {gen_stats['unique_chars']} ç§å­—ç¬¦ (çœŸå®: {real_dga_stats['unique_chars']})")
    if asr < 40:
        print(f"  - ASRè¾ƒä½ ({asr:.1f}%)ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    print(f"\nğŸ¯ ä¼˜åŒ–å»ºè®®:")
    if asr < 50:
        print(f"  1. ç»§ç»­è®­ç»ƒè‡³æ›´å¤šè½®æ¬¡ (å½“å‰å¯èƒ½ä»…10è½®)")
        print(f"  2. è°ƒæ•´ç”Ÿæˆå™¨æ¶æ„ï¼Œå¢åŠ hidden_dimæˆ–å¢åŠ LSTMå±‚æ•°")
        print(f"  3. å°è¯•ä¸åŒçš„å­¦ä¹ ç‡ (å½“å‰1e-4)")
    if gen_stats['avg_length'] < real_dga_stats['avg_length'] * 0.7:
        print(f"  4. åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥é•¿åº¦æƒ©ç½šï¼Œé¼“åŠ±ç”Ÿæˆæ›´é•¿åºåˆ—")
    
    # ç›¸ä¼¼åº¦è¯„ä¼°
    length_similarity = 1 - abs(gen_stats['avg_length'] - real_dga_stats['avg_length']) / real_dga_stats['avg_length']
    char_similarity = gen_stats['unique_chars'] / real_dga_stats['unique_chars']
    
    print(f"\nğŸ“ˆ ä¸çœŸå®DGAç›¸ä¼¼åº¦:")
    print(f"  - é•¿åº¦ç›¸ä¼¼åº¦: {length_similarity*100:.2f}%")
    print(f"  - å­—ç¬¦å¤šæ ·æ€§ç›¸ä¼¼åº¦: {char_similarity*100:.2f}%")
    print(f"  - ç»¼åˆè¯„åˆ†: {(length_similarity + char_similarity + asr/100)/3*100:.2f}%")

if __name__ == "__main__":
    main()
