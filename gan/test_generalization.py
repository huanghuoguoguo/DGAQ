#!/usr/bin/env python
"""æµ‹è¯•å¯¹æŠ—è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼šç”¨ä¸åŒepochçš„ç”Ÿæˆå™¨æ”»å‡»"""

import os
import sys
import torch
import argparse

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from core.model.cnn_model import LightweightCNN
from core.model.cnn_moe_model import LightweightCNNMoE
from core.adversarial.generator import DGAGenerator
from core.dataset import load_dataset

def load_target_model(model_path, device, dataset_info):
    """åŠ è½½ç›®æ ‡åˆ†ç±»å™¨"""
    if 'moe' in model_path.lower():
        state = torch.load(model_path, map_location='cpu', weights_only=False)
        expert_keys = [k for k in state.keys() if k.startswith('experts.')]
        num_experts = 3
        if expert_keys:
            max_expert = max([int(k.split('.')[1]) for k in expert_keys])
            num_experts = max_expert + 1
        
        model = LightweightCNNMoE(
            vocab_size=dataset_info['vocab_size'],
            embedding_dim=128,
            max_length=dataset_info['max_length'],
            num_classes=dataset_info['num_classes'],
            num_experts=num_experts
        ).to(device)
    else:
        model = LightweightCNN(
            vocab_size=dataset_info['vocab_size'],
            embedding_dim=128,
            max_length=dataset_info['max_length'],
            num_classes=dataset_info['num_classes']
        ).to(device)
    
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device, weights_only=False))
    model.eval()
    return model

def load_generator(model_path, device, config):
    """åŠ è½½ç”Ÿæˆå™¨"""
    generator = DGAGenerator(
        vocab_size=config['vocab_size'],
        hidden_dim=config['hidden_dim'],
        max_len=config['max_len'],
        z_dim=config['z_dim']
    ).to(device)
    
    if os.path.exists(model_path):
        generator.load_state_dict(torch.load(model_path, map_location=device, weights_only=False))
    generator.eval()
    return generator

def evaluate_attack(generator, target_model, device, num_samples):
    """è¯„ä¼°æ”»å‡»æ•ˆæœ"""
    with torch.no_grad():
        adv_indices = generator.sample(num_samples, device)
        logits = target_model(adv_indices)
        probs = torch.softmax(logits, dim=1)
        predictions = torch.argmax(probs, dim=1)
        
        successful_evasions = (predictions == 0).sum().item()
        asr = successful_evasions / num_samples
        
        return {
            'asr': asr,
            'evasions': successful_evasions,
            'detections': num_samples - successful_evasions
        }

def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # é…ç½®
    dataset_path = './data/processed/500k_unified_dga_dataset.pkl'
    num_samples = 2000
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    try:
        from core.dataset import create_data_loaders
        _, _, _, dataset_info = create_data_loaders(dataset_path, batch_size=32, task_type='binary')
    except:
        dataset_info = {'vocab_size': 41, 'max_length': 60, 'num_classes': 2}
    
    gen_config = {
        'vocab_size': dataset_info.get('vocab_size', 41),
        'hidden_dim': 256,
        'max_len': dataset_info.get('max_length', 60),
        'z_dim': 100
    }
    
    # æµ‹è¯•çš„æ¨¡å‹
    models = [
        ("åŸå§‹CNN", "./models/cnn_binary_model.pth"),
        ("å¯¹æŠ—è®­ç»ƒCNN", "./models/cnn_adversarial_trained.pth"),
        ("åŸå§‹CNN-MoE", "./models/cnn_moe_binary_model.pth"),
        ("å¯¹æŠ—è®­ç»ƒCNN-MoE", "./models/cnn_moe_adversarial_trained.pth"),
    ]
    
    # æµ‹è¯•çš„ç”Ÿæˆå™¨ï¼ˆä¸åŒepochï¼‰
    generator_epochs = [5, 10, 20, 30, 40, 50]
    
    print("="*80)
    print("æ³›åŒ–èƒ½åŠ›æµ‹è¯•ï¼šç”¨ä¸åŒepochçš„ç”Ÿæˆå™¨æ”»å‡»å¯¹æŠ—è®­ç»ƒå‰åçš„æ¨¡å‹")
    print("="*80)
    print()
    
    results = {}
    
    for model_name, model_path in models:
        if not os.path.exists(model_path):
            print(f"âš ï¸  è·³è¿‡ {model_name}ï¼ˆæ¨¡å‹ä¸å­˜åœ¨ï¼‰")
            continue
        
        print(f"\n{'='*80}")
        print(f"ç›®æ ‡æ¨¡å‹: {model_name}")
        print(f"{'='*80}")
        
        target_model = load_target_model(model_path, device, dataset_info)
        results[model_name] = {}
        
        for epoch in generator_epochs:
            gen_path = f"./models/gan/generator_epoch_{epoch}.pth"
            
            if not os.path.exists(gen_path):
                print(f"  Epoch {epoch:2d}: ç”Ÿæˆå™¨ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            generator = load_generator(gen_path, device, gen_config)
            metrics = evaluate_attack(generator, target_model, device, num_samples)
            
            results[model_name][epoch] = metrics
            
            print(f"  Epoch {epoch:2d}: ASR={metrics['asr']*100:5.2f}%, "
                  f"é€ƒé€¸={metrics['evasions']:4d}, æ£€æµ‹={metrics['detections']:4d}")
    
    # ç”Ÿæˆå¯¹æ¯”åˆ†æ
    print(f"\n\n{'='*80}")
    print("ğŸ“Š æ³›åŒ–èƒ½åŠ›åˆ†æ")
    print(f"{'='*80}\n")
    
    # å¯¹æ¯”è¡¨æ ¼
    print(f"{'ç”Ÿæˆå™¨Epoch':<15}", end='')
    for model_name in ["åŸå§‹CNN", "å¯¹æŠ—è®­ç»ƒCNN", "åŸå§‹CNN-MoE", "å¯¹æŠ—è®­ç»ƒCNN-MoE"]:
        if model_name in results:
            print(f"{model_name:>20}", end='')
    print()
    print("-"*80)
    
    for epoch in generator_epochs:
        print(f"Epoch {epoch:<8}", end='')
        for model_name in ["åŸå§‹CNN", "å¯¹æŠ—è®­ç»ƒCNN", "åŸå§‹CNN-MoE", "å¯¹æŠ—è®­ç»ƒCNN-MoE"]:
            if model_name in results and epoch in results[model_name]:
                asr = results[model_name][epoch]['asr'] * 100
                print(f"{asr:>19.2f}%", end='')
            else:
                print(f"{'N/A':>20}", end='')
        print()
    
    # å…³é”®å‘ç°
    print(f"\n{'='*80}")
    print("ğŸ” å…³é”®å‘ç°")
    print(f"{'='*80}\n")
    
    if "åŸå§‹CNN" in results and "å¯¹æŠ—è®­ç»ƒCNN" in results:
        print("CNNæ¨¡å‹å¯¹æ¯”ï¼š")
        for epoch in generator_epochs:
            if epoch in results["åŸå§‹CNN"] and epoch in results["å¯¹æŠ—è®­ç»ƒCNN"]:
                original_asr = results["åŸå§‹CNN"][epoch]['asr'] * 100
                trained_asr = results["å¯¹æŠ—è®­ç»ƒCNN"][epoch]['asr'] * 100
                reduction = original_asr - trained_asr
                print(f"  Epoch {epoch}: {original_asr:.1f}% â†’ {trained_asr:.1f}% "
                      f"(é™ä½ {reduction:.1f}%)")
        
        # è®¡ç®—å¹³å‡æ³›åŒ–èƒ½åŠ›
        avg_reduction = sum([
            results["åŸå§‹CNN"][e]['asr'] - results["å¯¹æŠ—è®­ç»ƒCNN"][e]['asr']
            for e in generator_epochs
            if e in results["åŸå§‹CNN"] and e in results["å¯¹æŠ—è®­ç»ƒCNN"]
        ]) / len([e for e in generator_epochs 
                  if e in results["åŸå§‹CNN"] and e in results["å¯¹æŠ—è®­ç»ƒCNN"]])
        
        print(f"\n  å¹³å‡ASRé™ä½: {avg_reduction*100:.1f}%")
        print(f"  å¯¹æŠ—è®­ç»ƒæ•ˆæœ: {'âœ… æ˜¾è‘—' if avg_reduction > 0.3 else 'âš ï¸  ä¸€èˆ¬' if avg_reduction > 0.1 else 'âŒ è¾ƒå¼±'}")

if __name__ == "__main__":
    main()
