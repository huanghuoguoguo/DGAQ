#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æMoEæ¨¡å‹çš„ä¸“å®¶ä½¿ç”¨æƒ…å†µ
æ£€æµ‹ä¸“å®¶å¡Œé™·é—®é¢˜
"""

import torch
import torch.nn as nn
from core.model.mamba2_moe_model import LightweightMamba2MoE
from core.data_loader import DGADataset
from torch.utils.data import DataLoader

def analyze_expert_usage(model, data_loader, device, num_batches=100):
    """åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µ"""
    model.eval()
    
    num_experts = model.num_experts
    num_layers = len(model.layers)
    
    # ç»Ÿè®¡æ¯å±‚æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨æ¬¡æ•°
    expert_counts = torch.zeros(num_layers, num_experts).to(device)
    total_tokens = 0
    
    with torch.no_grad():
        for batch_idx, (data, targets) in enumerate(data_loader):
            if batch_idx >= num_batches:
                break
                
            data = data.to(device)
            outputs, gate_info = model(data, return_gate=True)
            
            # ç»Ÿè®¡æ¯å±‚çš„ä¸“å®¶é€‰æ‹©
            for layer_idx, gate_weights in enumerate(gate_info['gate_weights']):
                # gate_weights: [B*L, num_experts]
                top_idx = torch.argmax(gate_weights, dim=-1)  # [B*L]
                for expert_idx in range(num_experts):
                    expert_counts[layer_idx, expert_idx] += (top_idx == expert_idx).sum()
                
                total_tokens += gate_weights.size(0)
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    expert_percentages = expert_counts / (total_tokens / num_layers) * 100
    
    return expert_percentages.cpu().numpy()

def print_expert_analysis(expert_percentages):
    """æ‰“å°ä¸“å®¶ä½¿ç”¨åˆ†æ"""
    num_layers, num_experts = expert_percentages.shape
    expected = 100.0 / num_experts
    
    print("\n" + "="*70)
    print("ğŸ“Š MoE ä¸“å®¶ä½¿ç”¨æƒ…å†µåˆ†æ")
    print("="*70)
    
    for layer_idx in range(num_layers):
        print(f"\nç¬¬ {layer_idx+1} å±‚:")
        print(f"  æœŸæœ›ä½¿ç”¨ç‡: {expected:.2f}% (å‡è¡¡çŠ¶æ€)")
        print(f"  å®é™…ä½¿ç”¨ç‡:")
        
        for expert_idx in range(num_experts):
            usage = expert_percentages[layer_idx, expert_idx]
            deviation = abs(usage - expected)
            
            # å¯è§†åŒ–æ¡å½¢å›¾
            bar_len = int(usage / 2)  # é™¤ä»¥2æ˜¯ä¸ºäº†é€‚åº”50å­—ç¬¦å®½åº¦
            bar = 'â–ˆ' * bar_len + 'â–‘' * (50 - bar_len)
            
            # é¢œè‰²æ ‡è®°ï¼ˆç”¨emojiè¡¨ç¤ºï¼‰
            if usage < 5:
                status = "ğŸ”´ å¡Œé™·"
            elif deviation > 20:
                status = "ğŸŸ¡ ä¸å‡è¡¡"
            elif deviation > 10:
                status = "ğŸŸ¢ åå·®"
            else:
                status = "âœ… æ­£å¸¸"
            
            print(f"    ä¸“å®¶ {expert_idx+1}: [{bar}] {usage:5.2f}% {status}")
    
    # æ•´ä½“åˆ†æ
    print("\n" + "-"*70)
    print("æ•´ä½“åˆ†æ:")
    
    # æ£€æµ‹å¡Œé™·
    collapsed = (expert_percentages < 5).any(axis=0)
    if collapsed.any():
        collapsed_experts = [i+1 for i, c in enumerate(collapsed) if c]
        print(f"  âš ï¸  ä¸“å®¶å¡Œé™·: ä¸“å®¶ {collapsed_experts} å‡ ä¹ä¸è¢«ä½¿ç”¨ (<5%)")
    else:
        print(f"  âœ… æ— ä¸“å®¶å¡Œé™·")
    
    # æ£€æµ‹è´Ÿè½½ä¸å‡è¡¡
    max_imbalance = abs(expert_percentages - expected).max()
    print(f"  æœ€å¤§è´Ÿè½½åå·®: {max_imbalance:.2f}%")
    
    if max_imbalance > 30:
        print(f"  âš ï¸  ä¸¥é‡ä¸å‡è¡¡! å»ºè®®é™ä½ balance_weight")
    elif max_imbalance > 20:
        print(f"  ğŸŸ¡ è½»å¾®ä¸å‡è¡¡ï¼Œå¯ä»¥ä¼˜åŒ–")
    else:
        print(f"  âœ… è´Ÿè½½å‡è¡¡è‰¯å¥½")
    
    # è®¡ç®—æ–¹å·®
    variance = expert_percentages.var(axis=1).mean()
    print(f"  å¹³å‡æ–¹å·®: {variance:.2f}")
    
    print("="*70)

if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®é›†...")
    dataset = DGADataset()
    dataset.load('/jsj_ywj/yhh/DGAQ/data/processed/500k_unified_dga_dataset.pkl')
    
    _, val_loader, _ = dataset.get_loaders(batch_size=512, task_type='binary')
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºMamba2-MoEæ¨¡å‹...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = LightweightMamba2MoE(
        vocab_size=dataset.vocab_size,
        embedding_dim=256,
        max_length=dataset.max_length,
        num_classes=2,
        num_layers=2,
        d_state=128,
        headdim=64,
        num_experts=4,
        expert_hidden=256,
        dropout_rate=0.15,
        balance_weight=0.001
    ).to(device)
    
    # å°è¯•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    try:
        model.load_state_dict(torch.load('./models/mamba2_moe_binary_model.pth'))
        print("âœ… åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
    except:
        print("âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    # åˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µ
    print("\nåˆ†æä¸“å®¶ä½¿ç”¨æƒ…å†µï¼ˆéªŒè¯é›†å‰100æ‰¹æ¬¡ï¼‰...")
    expert_percentages = analyze_expert_usage(model, val_loader, device, num_batches=100)
    
    # æ‰“å°åˆ†æç»“æœ
    print_expert_analysis(expert_percentages)
